{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering algorithm\n",
    "\"\"\"\n",
    "Created on Tue Oct  3 21:49:05 2023\n",
    "\n",
    "@author: hkpen edited from Anupam University of Manchester\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import_complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import sys\n",
    "from sklearn import manifold\n",
    "from sklearn import decomposition\n",
    "from sklearn import metrics\n",
    "from functools import partial\n",
    "import hdbscan\n",
    "#from s_dbw import S_Dbw\n",
    "#from internal_validation import internalIndex\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas\n",
    "import networkx as nx\n",
    "#import seaborn as sns\n",
    "#from scipy.spatial.distance import euclidean\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "print ('import_complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithm below - much of the code is commented out by Anupam but it functions, simply with a csv file of the fingerprints (each row of numbers should be one material's fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X has duplicates: False\n",
      "modified X has duplicates: False\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 100\u001b[0m\n\u001b[0;32m     95\u001b[0m dbcv_score\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mn_clusters_)]        \u001b[38;5;66;03m########## when DBCV can't be calculated\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m##### write summary\u001b[39;00m\n\u001b[0;32m     97\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(min_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(min_samp)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(n_clusters_)\u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(n_noise_)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mmax\u001b[39m(cluster_persistence))\u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39maverage(cluster_persistence))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mmax\u001b[39m(plot_obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar_tops\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m--> 100\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mmax\u001b[39m(hist_labels_new))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(metrics\u001b[38;5;241m.\u001b[39msilhouette_score(X, labels))\u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m    101\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m   \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mcalinski_harabasz_score(X, labels))\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mdavies_bouldin_score(X, labels))\u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m    102\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(dbcv_score[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(sdbw_score)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m################## plot condensed tree\u001b[39;00m\n\u001b[0;32m    105\u001b[0m figure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m25\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:131\u001b[0m, in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:258\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    189\u001b[0m     {\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msilhouette_samples\u001b[39m(X, labels, \u001b[38;5;241m*\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the Silhouette Coefficient for each sample.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    The Silhouette Coefficient is a measure of how well samples are clustered\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m       <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     X, labels \u001b[38;5;241m=\u001b[39m check_X_y(X, labels, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Check for non-zero diagonal entries in precomputed distance matrix\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN."
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "####  Reading input fingerprints\n",
    "###################################\n",
    "from numpy import genfromtxt\n",
    "X= genfromtxt('flat_mat_DOS_fingerprints.csv', delimiter=',')\n",
    "#indices=genfromtxt('../Reduced_lattice_fingerprint_from_DOS/input_indices_244.csv', delimiter=',')\n",
    "#sublattice = np.transpose(pandas.read_csv('../Reduced_lattice_fingerprint_from_DOS/sublattice_element_244.csv', header=None).values)[0]\n",
    "#database_index=indices.astype(int)\n",
    "\n",
    "\n",
    "################################################################\n",
    "####  Read all materials data with flatness prediction\n",
    "################################\n",
    "#data = pandas.read_csv('../../write_and_plot_clusters_calculate_flatness_score/All_mat_new_test_score_with_horz_flat_index.csv')\n",
    "\n",
    "#######################################\n",
    "#### Removing duplicates by adding a small random number\n",
    "def contains_duplicates(X):\n",
    "    return len(np.unique(X,axis=0)) != len(X)\n",
    "\n",
    "print('X has duplicates: '+str(contains_duplicates(X)))\n",
    "\n",
    "if contains_duplicates(X):\n",
    "    ##### identifying idices of duplicates\n",
    "    unq, count = np.unique(X, axis=0, return_counts=True)\n",
    "    repeated_groups = unq[count > 1]\n",
    "    for repeated_group in repeated_groups:\n",
    "        repeated_idx = np.argwhere(np.all(X == repeated_group, axis=1))\n",
    "        AA=repeated_idx.ravel()\n",
    "        #print(AA)\n",
    "        for p in AA:\n",
    "            for q in range(len(X[p])):\n",
    "                if X[p,q]>0:\n",
    "                    bb=X[p,q]\n",
    "                    X[p,q]=bb+(np.random.rand(1)[0])/1e5\n",
    "\n",
    "print('modified X has duplicates: '+str(contains_duplicates(X)))\n",
    "# max_fingerprint=X.max(axis=1)\n",
    "# min_fingerprint=X.min(axis=1)\n",
    "# print(max_fingerprint-min_fingerprint)\n",
    "\n",
    "\n",
    "###################################\n",
    "####  HDBSCAN\n",
    "MS=5\n",
    "SS=5\n",
    "for min_size in range(MS,MS+1,1):\n",
    "    #print(min_size)\n",
    "    for min_samp in range(SS,SS+1,1):\n",
    "\n",
    "        fname='HDBSCAN_244_DOS_minsize_'+str(min_size)+'_minsamp_'+str(min_samp)\n",
    "        f = open(fname+'_summary_india.txt', 'w')\n",
    "        f.write('Minkowski_metric_p=0.2\\n')\n",
    "        f.write('Total#sample   Min_size   Min_samples   N_clusters   N_noise  Max_persistence  Avg_persistence \\\n",
    "                Max_Lambda_in_bar  Max_cluster_size  Silhouette  CH_measure DB_measure DBCV s-dbw \\n')\n",
    "\n",
    "        hdb=hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\\\n",
    "                        gen_min_span_tree=False, leaf_size=40, metric='minkowski', cluster_selection_method='eom', min_cluster_size=min_size, min_samples=min_samp, p=0.2)\n",
    "        db=hdb.fit(X)\n",
    "        labels = db.labels_\n",
    "\n",
    "        #################\n",
    "        #### all cluster properties\n",
    "        n_clusters_ = db.labels_.max()+1\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "        hist_labels=np.unique(labels, return_counts=True)  ### calculate max cluster size\n",
    "        hist_labels_new=np.delete(hist_labels[1],0)  ### calculate max cluster size\n",
    "        probabilities=db.probabilities_\n",
    "        cluster_persistence=db.cluster_persistence_\n",
    "        examples=db.exemplars_\n",
    "        outlier_score=db.outlier_scores_\n",
    "        ######calculate cluster size\n",
    "        unique_label,cluster_rep_index, counts = np.unique(labels, return_index=True, return_counts=True)\n",
    "        cluster_size=[]\n",
    "        for x in labels:\n",
    "            cluster_size.append(counts[np.where(unique_label==x)[0][0]])\n",
    "        cluster_size=np.array(cluster_size)\n",
    "\n",
    "\n",
    "\n",
    "        #################\n",
    "        #### plot objects\n",
    "        cond_tree=db.condensed_tree_\n",
    "        plot_obj=cond_tree.get_plot_data()\n",
    "        #single_link_tree=db.single_linkage_tree_\n",
    "\n",
    "\n",
    "        ##################\n",
    "        #### validation indices\n",
    "        sdbw_score='unknown'\n",
    "        #sdbw_score = S_Dbw(X, labels, centers_id=None, method='Tong', alg_noise='bind',centr='mean', nearest_centr=True, metric='euclidean')\n",
    "        #print(score)\n",
    "        #cdbw_score = CDbw(X, labels,)\n",
    "        #dbcv_score=hdbscan.validity.validity_index(X,labels,metric='minkowski',per_cluster_scores=True)\n",
    "        dbcv_score= [0,np.array([0]*n_clusters_)]        ########## when DBCV can't be calculated\n",
    "        ##### write summary\n",
    "        f.write(str(len(X))+'   '+str(min_size)+'   '+str(min_samp)+'   '+str(n_clusters_)+\\\n",
    "                '   '+str(n_noise_)+'   '+str(max(cluster_persistence))+\\\n",
    "                    '   '+str(np.average(cluster_persistence))+'   '+str(max(plot_obj['bar_tops']))+\\\n",
    "                        '   '+str(max(hist_labels_new))+'   '+str(metrics.silhouette_score(X, labels))+\\\n",
    "                            '   '+str(metrics.calinski_harabasz_score(X, labels))+' '+str(metrics.davies_bouldin_score(X, labels))+\\\n",
    "                                '  '+str(dbcv_score[0])+'  '+str(sdbw_score)+'\\n')\n",
    "\n",
    "        ################## plot condensed tree\n",
    "        figure(figsize=(40, 25), dpi=100)\n",
    "        one=cond_tree.plot(leaf_separation=0.5, cmap='plasma', select_clusters=False, label_clusters=True, selection_palette=None, axis=None, colorbar=True, log_size=True, max_rectangles_per_icicle=1)\n",
    "        plt.ylim((1000,1))\n",
    "        plt.yscale('log')\n",
    "        one.grid(True)\n",
    "        cond_plot_fname=fname+'_cond_tree_india.png'\n",
    "        plt.savefig(cond_plot_fname, bbox_inches='tight')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        ############## Colormap\n",
    "        ##############################\n",
    "        import matplotlib\n",
    "        cmap = plt.cm.get_cmap('turbo')\n",
    "        norm = matplotlib.colors.Normalize(vmin=min(labels), vmax=max(labels))\n",
    "\n",
    "        # #######################################\n",
    "        # #### Print detail outputs for each materials\n",
    "        # count=0\n",
    "        # data_all=[]\n",
    "        # for i in range(len(database_index)):\n",
    "        #     name='2dm-'+ str(database_index[i])\n",
    "        #     ind=data.index[data['ID'] == name][0]\n",
    "        #     data_all.append(data.loc[ind].values)\n",
    "        # data_all=np.array(data_all)\n",
    "        # clubbed_data=np.transpose(np.vstack((np.transpose(data_all),labels,cluster_size,sublattice,dbcv_score[1][labels],probabilities,cluster_persistence[labels],outlier_score)))\n",
    "        # Out=pandas.DataFrame(data=clubbed_data,columns=data.columns.to_list()+['cluster_index','cluster_size','sublattice_element','dbcv_cluster','probabilities','cluster_persistence','outlier_score'])\n",
    "        # Out.to_csv(fname+'_detailed_output_india.csv')\n",
    "\n",
    "        # ########################################\n",
    "        # #### Print details of each clusters  only\n",
    "        # clusters=[]\n",
    "        # for j in range(len(unique_label)):\n",
    "        #     if unique_label[j]==-1:\n",
    "        #         nn=np.array([unique_label[j],counts[j],0.000000,0.00000])\n",
    "        #     else:\n",
    "        #         nn=np.array([int(unique_label[j]),int(counts[j]),cluster_persistence[j-1],dbcv_score[1][j-1]])\n",
    "        #     clusters.append(nn)\n",
    "        # Out1=pandas.DataFrame(data=np.array(clusters),columns=['cluster_index','cluster_size','cluster_persistence','cluster_dbcv',])\n",
    "        # Out1.to_csv(fname+'_cluster_detail_india.csv')\n",
    "\n",
    "\n",
    "        # #####################################\n",
    "        # ###### get networkx tree\n",
    "        # #####################################\n",
    "        #Nx=cond_tree.to_networkx()\n",
    "        #print(nx.is_tree(Nx))\n",
    "        #print(nx.to_prufer_sequence(Nx))\n",
    "        #nx.to_edgelist(Nx, \"test.edgelist\")\n",
    "        #print(len(G.nodes(\"size\")))\n",
    "        #print(G.graph)\n",
    "        #print(len(G.edges))\n",
    "        #G1=G.nodes(\"size\")\n",
    "        #G1=G.hide_nodes()\n",
    "        #G = nx.Graph()\n",
    "        #print(G)\n",
    "        #subax1 = plt.subplot(121)\n",
    "        #options={'node_color':'blue','node_size':1,'width':1,'arrowstyle':'-|>','arrowsize':1}\n",
    "        #nx.draw_networkx(G, with_labels=False,arrows=True, **options)\n",
    "        #nx.draw(G, with_labels=False, font_weight='bold')\n",
    "        #plt.savefig('nx_plot_graph.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        ###### Pandas data\n",
    "        ##################################\n",
    "        panda_data=cond_tree.to_pandas()\n",
    "        #print(G.number_of_nodes())\n",
    "        #print(panda_data)\n",
    "        selected_clusters=cond_tree._select_clusters()\n",
    "        G1 = panda_data[panda_data['child_size'] > 1]\n",
    "        #New_Nx=nx.from_pandas_edgelist(G1,'parent','child',['lambda_val', 'child_size'])\n",
    "        #nx.write_edgelist(New_Nx,'New_edgelist', encoding = 'latin-1')\n",
    "        ################################\n",
    "        ################### To list from pandas\n",
    "        len_G1=[]\n",
    "        cluster_id=[]\n",
    "        for ind1 in G1.index:\n",
    "            len_G1.append(0.1)\n",
    "            if G1.at[ind1,'child'] in selected_clusters:\n",
    "                cluster_id.append(str(selected_clusters.index(G1.at[ind1,'child'])))\n",
    "            else:\n",
    "                cluster_id.append('-1')\n",
    "        print(cluster_id)\n",
    "        G1.insert(4, 'dist_G1', len_G1)\n",
    "        G1.insert(5, 'cluster_id', cluster_id)\n",
    "        G2=G1.copy()\n",
    "        print(G2)\n",
    "        del G1['cluster_id']\n",
    "        del G1['lambda_val']\n",
    "        del G1['child_size']\n",
    "        g1_list=G1.values.tolist()\n",
    "        ##############################\n",
    "        ################ ETE treee from parent child relations\n",
    "        from ete3 import Tree,TreeStyle,NodeStyle\n",
    "        tree = Tree.from_parent_child_table(g1_list)\n",
    "        #tree.write(format=9,outfile='new_tree.nw')\n",
    "        print(G2)\n",
    "        for node in tree.traverse():\n",
    "            nstyle = NodeStyle()\n",
    "            if node.is_leaf():\n",
    "                index1=G2.index[G2['child'] == int(node.name)]\n",
    "                node.name=G2.at[index1[0],'cluster_id']\n",
    "                #nstyle = NodeStyle()\n",
    "                #print(int(node.name))\n",
    "                #print(matplotlib.colors.rgb2hex(cmap(norm(int(node.name)))))\n",
    "                nstyle[\"fgcolor\"] = str(matplotlib.colors.rgb2hex(cmap(norm(int(node.name)))))\n",
    "                #nstyle['fgcolor']='#FF0000'\n",
    "                nstyle[\"size\"] = G2.at[index1[0],'child_size']/2\n",
    "            else:\n",
    "                nstyle[\"fgcolor\"] ='black'\n",
    "            node.set_style(nstyle)\n",
    "        tree.write(format=1,outfile='new_tree.nw')\n",
    "        #################################\n",
    "        ################### Plot\n",
    "        ts = TreeStyle()\n",
    "        ts.mode='c'\n",
    "        ts.arc_start = -180 # 0 degrees = 3 o'clock\n",
    "        ts.arc_span = 360\n",
    "        ts.scale = 40\n",
    "        ts.show_leaf_name=True\n",
    "        tree.show(tree_style=ts)\n",
    "\n",
    "\n",
    "        # #####################################\n",
    "        # ###### Plot sunburst chart\n",
    "        # #####################################\n",
    "        # print(G.size)\n",
    "        # print(G1.size)\n",
    "        # import math\n",
    "        # import plotly.graph_objects as go\n",
    "        # labels1=G['child'].astype(str).values.tolist()\n",
    "        # parent1=G['parent'].astype(str).values.tolist()\n",
    "        # valuess1=G['child_size'].values.tolist()\n",
    "        # logvalues1 = [math.log(x) for x in valuess1]\n",
    "        # fig =go.Figure(go.Sunburst(\n",
    "        #         labels=labels1,\n",
    "        #         parents=parent1,\n",
    "        #         values=logvalues1,\n",
    "        # ))\n",
    "        # fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n",
    "        # fig.write_image(\"sunburst.png\", width=600, height=350, scale=2)\n",
    "\n",
    "        # #####################################\n",
    "        # ###### Circular tree plot\n",
    "        # ##################################\n",
    "        # from ete3 import Tree, TreeStyle\n",
    "        # t = Tree()\n",
    "        # t.populate(30)\n",
    "        # ts = TreeStyle()\n",
    "        # ts.show_leaf_name = True\n",
    "        # ts.mode = \"c\"\n",
    "        # ts.arc_start = -180 # 0 degrees = 3 o'clock\n",
    "        # ts.arc_span = 180\n",
    "        # t.render(\"circular_tree.png\", w=183, units=\"mm\")\n",
    "\n",
    "\n",
    "        ########################################\n",
    "        ##### Plot condensed tree\n",
    "        ###########################\n",
    "        # create a colormap\n",
    "        #colors = [(1, 0, 0), (0.5, 0.5, 0.5), (0, 0, 1)]  # R -> G -> B\n",
    "        #n_bins = 500  # Discretizes the interpolation into bins\n",
    "        #cmap_name = 'my_list'\n",
    "        #cmap = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\n",
    "        #figure(figsize=(40, 20), dpi=400)\n",
    "        #one=cond_tree.plot(leaf_separation=0.5, cmap='plasma', select_clusters=False, label_clusters=True, selection_palette=None, axis=None, colorbar=True, log_size=True, max_rectangles_per_icicle=1)\n",
    "        #plt.ylim((1000,1))\n",
    "        #plt.yscale('log')\n",
    "        #one.grid(True)\n",
    "        #cond_plot_fname='HDB_'+str(MS)+'_'+str(SS)+'.png'\n",
    "        #plt.savefig(cond_plot_fname, bbox_inches='tight')\n",
    "\n",
    "        #one.savefig(os.path.basename(filename_bands).split('.')[0] + '.bsdos.png',dpi=200)\n",
    "        #one.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # import plotly.express as px\n",
    "        # fig =px.icicle(plot_obj)\n",
    "        # fig.show()\n",
    "\n",
    "        ###################################\n",
    "        ######### icicle plot : lambda axis is from left to right\n",
    "        # from matplotlib.patches import Rectangle\n",
    "        # x_centers=np.array(plot_obj['bar_centers'])\n",
    "        # lam_max=np.array(plot_obj['bar_tops'])\n",
    "        # lam_min=np.array(plot_obj['bar_bottoms'])\n",
    "        # x_width=np.array(plot_obj['bar_widths'])\n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # for i in range(len(x_centers)):\n",
    "        #     ax.add_patch(Rectangle((lam_min[i], x_centers[i]-(x_width[i]/2)), lam_max[i]-lam_min[i], x_width[i],color=\"red\"))\n",
    "        # plt.xlim((0.00002,10000))\n",
    "        # plt.ylim((0,85000))\n",
    "        # plt.xscale('log')\n",
    "        # plt.show()\n",
    "\n",
    "        ##################################\n",
    "        ##### Pandas icicle plot\n",
    "        # import pandas as pd\n",
    "        # import plotly.express as px\n",
    "\n",
    "        # count=0\n",
    "        # label=[]\n",
    "        # parents=[]\n",
    "        # child_size=[]\n",
    "        # for i in range(len(cond_tree_pandas)):\n",
    "        #     if cond_tree_pandas.loc[i].child_size >= 1:\n",
    "        #         label.append(str(int(cond_tree_pandas.loc[i].name)))\n",
    "        #         parents.append(str(int(cond_tree_pandas.loc[i].parent)))\n",
    "        #         child_size.append(int(cond_tree_pandas.loc[i].child_size))\n",
    "\n",
    "        # df = pd.DataFrame(dict(parents=parents, label=label, child_size=child_size))\n",
    "        # df[\"all\"] = \"all\" # in order to have a single root node\n",
    "        # print(df)\n",
    "        # fig = px.icicle(df, path=['all', 'parents', 'label'], values='child_size')\n",
    "        # fig.update_traces(root_color='lightgrey')\n",
    "        # fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "        # fig.show()\n",
    "        # fig.write_image(\"hdb_icicles.png\")\n",
    "\n",
    "\n",
    "\n",
    "        ##################################\n",
    "        ######## tSNE visualization 3D\n",
    "#         tsne=manifold.TSNE(n_components=3, verbose=1, perplexity=50, n_iter=300)\n",
    "#         ts= tsne.fit_transform(X)\n",
    "#         fig = plt.figure(figsize=(12, 12))\n",
    "#         ax = fig.add_subplot(projection='3d')\n",
    "#         ########\n",
    "#         ##### for different eps\n",
    "#         # for i in np.arange(0.025,0.31,0.025):\n",
    "#         #     eps=round(i, 3)\n",
    "#         #     label=db.single_linkage_tree_.get_clusters(eps, min_cluster_size=3)\n",
    "#         #     n_clusters_ = label.max()\n",
    "#         #     n_noise_ = list(label).count(-1)\n",
    "#         #     print(str(eps)+' '+str(len(labels))+' '+str(n_clusters_)+' '+str(n_noise_)+' '+str(metrics.silhouette_score(X, label)))\n",
    "#         #     ax.scatter(ts[:,0], ts[:,1], ts[:,2], c=labels+1, cmap=\"hsv\")\n",
    "#         #     fname=\"t-SNE result_at_eps\"+str(eps)\n",
    "#         #     plt.title(fname)\n",
    "#         #     plt.savefig(fname+'.png', bbox_inches='tight')\n",
    "#         #     plt.show()\n",
    "#         ###########\n",
    "#         ###### for current minsize and minsamples\n",
    "#         ax.scatter(ts[:,0], ts[:,1], ts[:,2], c=labels+1, cmap=\"hsv\")\n",
    "#         fname=\"t-SNE result_at_minsize_\"+str(min_size)+\"_minsamp_\"+str(min_samp)\n",
    "#         plt.title(fname)\n",
    "#         plt.savefig(fname+'.png', bbox_inches='tight')\n",
    "#         plt.show()\n",
    "\n",
    "        ######################################\n",
    "        ########## PCA, MDS, SE, LLE, Mod LLE,  tSNE etc 2D\n",
    "        n_components=2\n",
    "        n_neighbors=500\n",
    "        #pca=decomposition.PCA(n_components=2)\n",
    "        #LLE = partial(manifold.LocallyLinearEmbedding,n_neighbors=n_neighbors,n_components=n_components,eigen_solver=\"auto\",)\n",
    "        #lle = LLE(method=\"standard\")\n",
    "        #LTSA = LLE(method=\"ltsa\")\n",
    "        #H_LLE = LLE(method=\"hessian\")\n",
    "        #M_LLE = LLE(method=\"modified\")\n",
    "        #Iso_map = manifold.Isomap(n_neighbors=n_neighbors, n_components=n_components)\n",
    "        #mds = manifold.MDS(n_components, max_iter=1000, n_init=1)\n",
    "        #se = manifold.SpectralEmbedding(n_components=n_components, n_neighbors=n_neighbors)\n",
    "        tsne = manifold.TSNE(n_components=n_components, early_exaggeration=12.0,init=\"pca\",learning_rate=100, random_state=0,perplexity=30,n_iter=10000,verbose=2)\n",
    "        objct= tsne.fit_transform(X)\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        s=np.ones((len(labels),1))*5\n",
    "        s[labels==-1]=0.2\n",
    "        c=labels\n",
    "        #c[labels==-1]=-5\n",
    "        plt.scatter(objct[:,0], objct[:,1],s=s, c=c*5, cmap=\"turbo\")\n",
    "        #plt.scatter(objct[:,0], objct[:,1],s=s, c='k')\n",
    "        #import matplotlib\n",
    "        for rep_id in cluster_rep_index:\n",
    "            #print(rep_id)\n",
    "            #cmap = plt.cm.get_cmap('turbo')\n",
    "            #norm = matplotlib.colors.Normalize(vmin=min(labels), vmax=max(labels))\n",
    "            col=cmap(norm(labels[rep_id]))\n",
    "            #print(col)\n",
    "            #plt.annotate(labels[rep_id],objct[rep_id,:]+np.random.uniform(low=-2, high=2, size=2),color='r',alpha=0.7, weight='normal', ha='center', va='center', size=10)\n",
    "            plt.annotate(labels[rep_id],objct[rep_id,:]+[3.5,0],color=col,alpha=1, weight='normal', ha='center', va='center', size=14).draggable()\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "        ax.axis('off')\n",
    "        plt.rc('font', size=15)          # controls default text sizes\n",
    "        plt.rc('figure', titlesize=18)  # fontsize of the figure title\n",
    "        #plt.title('t-SNE 2D visualization of the fingerprint space')\n",
    "        plt.show()\n",
    "        plt.savefig(fname+'_tsne_new_india.svg', format = 'svg', dpi=300)\n",
    "        plt.savefig(fname+'_tsne_new_india.png', bbox_inches='tight')\n",
    "        # plt.show()\n",
    "        plt.close(fig=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
